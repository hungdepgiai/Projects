{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import required packages\r\n",
    "import cv2\r\n",
    "import tensorflow as tf\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\r\n",
    "from keras.optimizers import Adam\r\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Initialize image data generator with rescaling\r\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)\r\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Preprocess all train images\r\n",
    "train_generator = train_data_gen.flow_from_directory(\r\n",
    "        'data/train',\r\n",
    "        target_size=(48, 48),\r\n",
    "        batch_size=64,\r\n",
    "        color_mode=\"grayscale\",\r\n",
    "        class_mode='categorical')\r\n",
    "\r\n",
    "# Preprocess all validation images\r\n",
    "validation_generator = validation_data_gen.flow_from_directory(\r\n",
    "        'data/test',\r\n",
    "        target_size=(48, 48),\r\n",
    "        batch_size=64,\r\n",
    "        color_mode=\"grayscale\",\r\n",
    "        class_mode='categorical')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# create model structure\r\n",
    "emotion_model = Sequential()\r\n",
    "\r\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\r\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\r\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "emotion_model.add(Dropout(0.25))\r\n",
    "\r\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\r\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\r\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "emotion_model.add(Dropout(0.25))\r\n",
    "\r\n",
    "emotion_model.add(Flatten())\r\n",
    "emotion_model.add(Dense(1024, activation='relu'))\r\n",
    "emotion_model.add(Dropout(0.5))\r\n",
    "emotion_model.add(Dense(7, activation='softmax'))\r\n",
    "\r\n",
    "cv2.ocl.setUseOpenCL(False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])\r\n",
    "\r\n",
    "# Train the neural network/model\r\n",
    "emotion_model_info = emotion_model.fit(\r\n",
    "        train_generator,\r\n",
    "        steps_per_epoch=28709 // 64,\r\n",
    "        epochs=50,\r\n",
    "        validation_data=validation_generator,\r\n",
    "        validation_steps=7178 // 64)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 741s 2s/step - loss: 1.8060 - accuracy: 0.2559 - val_loss: 1.7369 - val_accuracy: 0.3345\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 511s 1s/step - loss: 1.6284 - accuracy: 0.3655 - val_loss: 1.5278 - val_accuracy: 0.4188\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 228s 509ms/step - loss: 1.5174 - accuracy: 0.4161 - val_loss: 1.4557 - val_accuracy: 0.4512\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 22s 49ms/step - loss: 1.4415 - accuracy: 0.4482 - val_loss: 1.3857 - val_accuracy: 0.4706\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 20s 45ms/step - loss: 1.3823 - accuracy: 0.4719 - val_loss: 1.3366 - val_accuracy: 0.4925\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 361s 808ms/step - loss: 1.3324 - accuracy: 0.4947 - val_loss: 1.3043 - val_accuracy: 0.5064\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 277s 620ms/step - loss: 1.2899 - accuracy: 0.5116 - val_loss: 1.2709 - val_accuracy: 0.5146\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 505s 1s/step - loss: 1.2497 - accuracy: 0.5271 - val_loss: 1.2279 - val_accuracy: 0.5336\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 474s 1s/step - loss: 1.2169 - accuracy: 0.5418 - val_loss: 1.2246 - val_accuracy: 0.5343\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 31s 69ms/step - loss: 1.1850 - accuracy: 0.5537 - val_loss: 1.1946 - val_accuracy: 0.5539\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.1545 - accuracy: 0.5688 - val_loss: 1.1666 - val_accuracy: 0.5597\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 365s 816ms/step - loss: 1.1270 - accuracy: 0.5782 - val_loss: 1.1594 - val_accuracy: 0.5621\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 563s 1s/step - loss: 1.0997 - accuracy: 0.5881 - val_loss: 1.1416 - val_accuracy: 0.5705\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 17s 39ms/step - loss: 1.0260 - accuracy: 0.6144 - val_loss: 1.1060 - val_accuracy: 0.5833\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 17s 39ms/step - loss: 1.0002 - accuracy: 0.6254 - val_loss: 1.1001 - val_accuracy: 0.5903\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.9800 - accuracy: 0.6355 - val_loss: 1.1064 - val_accuracy: 0.5840\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.9516 - accuracy: 0.6471 - val_loss: 1.0830 - val_accuracy: 0.5975\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.9293 - accuracy: 0.6567 - val_loss: 1.0856 - val_accuracy: 0.5985\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.9137 - accuracy: 0.6658 - val_loss: 1.0853 - val_accuracy: 0.5935\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 20s 44ms/step - loss: 0.8872 - accuracy: 0.6764 - val_loss: 1.0770 - val_accuracy: 0.6049\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.8637 - accuracy: 0.6819 - val_loss: 1.0847 - val_accuracy: 0.6059\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.8388 - accuracy: 0.6930 - val_loss: 1.0863 - val_accuracy: 0.6057\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.8113 - accuracy: 0.7045 - val_loss: 1.0871 - val_accuracy: 0.5999\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.7896 - accuracy: 0.7111 - val_loss: 1.0743 - val_accuracy: 0.6085\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 18s 39ms/step - loss: 0.7733 - accuracy: 0.7156 - val_loss: 1.0786 - val_accuracy: 0.6083\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 19s 42ms/step - loss: 0.7460 - accuracy: 0.7282 - val_loss: 1.0744 - val_accuracy: 0.6123\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.7246 - accuracy: 0.7346 - val_loss: 1.0813 - val_accuracy: 0.6137\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.7029 - accuracy: 0.7444 - val_loss: 1.0916 - val_accuracy: 0.6150\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 19s 42ms/step - loss: 0.6800 - accuracy: 0.7518 - val_loss: 1.0873 - val_accuracy: 0.6182\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.6603 - accuracy: 0.7617 - val_loss: 1.0898 - val_accuracy: 0.6169\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.6384 - accuracy: 0.7685 - val_loss: 1.0961 - val_accuracy: 0.6151\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.6144 - accuracy: 0.7753 - val_loss: 1.1044 - val_accuracy: 0.6191\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.5934 - accuracy: 0.7852 - val_loss: 1.1104 - val_accuracy: 0.6217\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 19s 42ms/step - loss: 0.5774 - accuracy: 0.7919 - val_loss: 1.1145 - val_accuracy: 0.6187\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.5579 - accuracy: 0.7988 - val_loss: 1.1103 - val_accuracy: 0.6211\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.5355 - accuracy: 0.8079 - val_loss: 1.1334 - val_accuracy: 0.6179\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.5143 - accuracy: 0.8166 - val_loss: 1.1486 - val_accuracy: 0.6200\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.5026 - accuracy: 0.8199 - val_loss: 1.1651 - val_accuracy: 0.6194\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.4909 - accuracy: 0.8212 - val_loss: 1.1489 - val_accuracy: 0.6239\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.4693 - accuracy: 0.8287 - val_loss: 1.1669 - val_accuracy: 0.6268\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.4585 - accuracy: 0.8352 - val_loss: 1.1606 - val_accuracy: 0.6282\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 19s 42ms/step - loss: 0.4374 - accuracy: 0.8413 - val_loss: 1.1738 - val_accuracy: 0.6274\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 19s 41ms/step - loss: 0.4219 - accuracy: 0.8484 - val_loss: 1.1853 - val_accuracy: 0.6254\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 18s 40ms/step - loss: 0.4147 - accuracy: 0.8503 - val_loss: 1.1863 - val_accuracy: 0.6274\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 18s 41ms/step - loss: 0.3962 - accuracy: 0.8578 - val_loss: 1.2105 - val_accuracy: 0.6237\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 19s 42ms/step - loss: 0.3829 - accuracy: 0.8611 - val_loss: 1.2202 - val_accuracy: 0.6221\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 21s 46ms/step - loss: 0.3732 - accuracy: 0.8649 - val_loss: 1.2296 - val_accuracy: 0.6277\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 232s 518ms/step - loss: 0.3657 - accuracy: 0.8670 - val_loss: 1.2436 - val_accuracy: 0.6239\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# save model structure in json file\r\n",
    "model_json = emotion_model.to_json()\r\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\r\n",
    "    json_file.write(model_json)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# save trained model weight in .h5 file\r\n",
    "emotion_model.save_weights('emotion_model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "interpreter": {
   "hash": "4744879b1ea232a6c1c0f721923f08269b4369f173b01bf42db1ade4d6cbcf01"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}